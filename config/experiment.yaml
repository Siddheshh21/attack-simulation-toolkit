attack_config:
  backdoor_trigger_size: 3
  byzantine_strategy: sign_flip
  free_ride_rounds: 2
  label_flip_ratio: 0.3
  scaling_factor: 2.0
  sybil_clients: 2
data_dir: data
min_samples: 1000  # Reduced from 2500 for faster data processing
model_params:
  bagging_fraction: 0.7  # Reduced from 0.8 for faster training
  bagging_freq: 10  # Increased from 5 for faster training
  boosting_type: gbdt
  feature_fraction: 0.7  # Reduced from 0.8 for faster training
  force_col_wise: true
  is_unbalance: true
  lambda_l1: 0.0
  lambda_l2: 0.0
  learning_rate: 0.15  # Increased from 0.1 for faster convergence
  max_bin: 128  # Reduced from 255 for faster training
  max_depth: 4  # Reduced from 5 for faster training
  metric: auc
  min_child_samples: 30  # Increased from 20 for faster training
  min_data_in_leaf: 30  # Increased from 20 for faster training
  num_leaves: 20  # Reduced from 31 for faster training
  num_threads: 4
  objective: binary
  reg_alpha: 0.0  # Reduced from 0.1 for faster training
  reg_lambda: 0.0  # Reduced from 0.1 for faster training
  verbose: -1
num_boost_rounds: 20  # Reduced from 50 for faster training
num_clients: 10
num_rounds: 3  # Reduced from 2 for faster training
output_dir: artifacts
